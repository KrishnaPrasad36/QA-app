{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7725577,"sourceType":"datasetVersion","datasetId":4513404},{"sourceId":13572,"sourceType":"modelInstanceVersion","modelInstanceId":11237},{"sourceId":13602,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":11237},{"sourceId":13653,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":11237}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch transformers pandas","metadata":{"id":"BHJEZtQ6WKo2","outputId":"64bf5095-83d2-46d5-d468-5171aaa7c301","execution":{"iopub.status.busy":"2024-03-04T07:01:04.001935Z","iopub.execute_input":"2024-03-04T07:01:04.002660Z","iopub.status.idle":"2024-03-04T07:01:17.675609Z","shell.execute_reply.started":"2024-03-04T07:01:04.002631Z","shell.execute_reply":"2024-03-04T07:01:17.674333Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.1.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install shutil","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:01:17.677969Z","iopub.execute_input":"2024-03-04T07:01:17.678749Z","iopub.status.idle":"2024-03-04T07:01:19.593823Z","shell.execute_reply.started":"2024-03-04T07:01:17.678707Z","shell.execute_reply":"2024-03-04T07:01:19.592668Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement shutil (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for shutil\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install accelerate -U","metadata":{"id":"txnD-14TX3pI","outputId":"4bdb74ea-4b7e-460c-b3eb-11855549c06f","execution":{"iopub.status.busy":"2024-03-04T07:01:19.595205Z","iopub.execute_input":"2024-03-04T07:01:19.595510Z","iopub.status.idle":"2024-03-04T07:01:32.265419Z","shell.execute_reply.started":"2024-03-04T07:01:19.595482Z","shell.execute_reply":"2024-03-04T07:01:32.264190Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments","metadata":{"id":"ixRUrbsRWaMH","execution":{"iopub.status.busy":"2024-03-04T07:01:32.268925Z","iopub.execute_input":"2024-03-04T07:01:32.269656Z","iopub.status.idle":"2024-03-04T07:01:51.137018Z","shell.execute_reply.started":"2024-03-04T07:01:32.269621Z","shell.execute_reply":"2024-03-04T07:01:51.136246Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-03-04 07:01:42.348944: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-04 07:01:42.349042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-04 07:01:42.493836: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/rajkishor/MajorFinal.csv')","metadata":{"id":"Rino7l21Weln","execution":{"iopub.status.busy":"2024-03-04T07:01:51.138100Z","iopub.execute_input":"2024-03-04T07:01:51.138636Z","iopub.status.idle":"2024-03-04T07:01:51.829684Z","shell.execute_reply.started":"2024-03-04T07:01:51.138611Z","shell.execute_reply":"2024-03-04T07:01:51.828690Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df['text']= df['Question'] + \" \" + df['Answer']\ndf['text'].to_csv('/kaggle/working/MajorFinaltrain1.txt', index=False, header=False)","metadata":{"id":"7T_IEFVjWmX1","execution":{"iopub.status.busy":"2024-03-04T07:01:51.830961Z","iopub.execute_input":"2024-03-04T07:01:51.831259Z","iopub.status.idle":"2024-03-04T07:01:53.145197Z","shell.execute_reply.started":"2024-03-04T07:01:51.831234Z","shell.execute_reply":"2024-03-04T07:01:53.144352Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:01:53.146249Z","iopub.execute_input":"2024-03-04T07:01:53.146530Z","iopub.status.idle":"2024-03-04T07:01:53.166838Z","shell.execute_reply.started":"2024-03-04T07:01:53.146506Z","shell.execute_reply":"2024-03-04T07:01:53.165972Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                   Topic                                           Question  \\\n0                 Dharan  What is there at Dharan to be considered as a ...   \n1                 Dharan              what should i try to visit in dharan?   \n2                 Dharan  What is the recommendation for the tourist fir...   \n3                 Dharan     tell me the best way to start the dharan tour?   \n4                 Dharan         What is the Dharan culture and traditions?   \n...                  ...                                                ...   \n50923  Trekking in Nepal  What challenges do trekkers face with transpor...   \n50924  Trekking in Nepal  How many distinct and diverse vegetation zones...   \n50925  Trekking in Nepal  Which rare species might be encountered in the...   \n50926  Trekking in Nepal  What accommodations are available on popular t...   \n50927  Trekking in Nepal  Why is hiring a porter considered valuable in ...   \n\n                                                  Answer  \\\n0      Dharan, located in the Koshi Zone of eastern N...   \n1      dharan in the koshi zone of eastern nepal is a...   \n2      For first-time visitors to Dharan, it is recom...   \n3      for first-time visitors to dharan it is recomm...   \n4      The Dharan Culture and traditions are deeply r...   \n...                                                  ...   \n50923  Transportation to the starting points of treks...   \n50924  Nepal has six distinct and diverse vegetation ...   \n50925  In the higher altitudes, one might encounter r...   \n50926  Lodge accommodation is available on popular tr...   \n50927  Hiring a porter is valued in Nepali culture be...   \n\n                                                    text  \n0      What is there at Dharan to be considered as a ...  \n1      what should i try to visit in dharan? dharan i...  \n2      What is the recommendation for the tourist fir...  \n3      tell me the best way to start the dharan tour?...  \n4      What is the Dharan culture and traditions? The...  \n...                                                  ...  \n50923  What challenges do trekkers face with transpor...  \n50924  How many distinct and diverse vegetation zones...  \n50925  Which rare species might be encountered in the...  \n50926  What accommodations are available on popular t...  \n50927  Why is hiring a porter considered valuable in ...  \n\n[50928 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dharan</td>\n      <td>What is there at Dharan to be considered as a ...</td>\n      <td>Dharan, located in the Koshi Zone of eastern N...</td>\n      <td>What is there at Dharan to be considered as a ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Dharan</td>\n      <td>what should i try to visit in dharan?</td>\n      <td>dharan in the koshi zone of eastern nepal is a...</td>\n      <td>what should i try to visit in dharan? dharan i...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dharan</td>\n      <td>What is the recommendation for the tourist fir...</td>\n      <td>For first-time visitors to Dharan, it is recom...</td>\n      <td>What is the recommendation for the tourist fir...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dharan</td>\n      <td>tell me the best way to start the dharan tour?</td>\n      <td>for first-time visitors to dharan it is recomm...</td>\n      <td>tell me the best way to start the dharan tour?...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dharan</td>\n      <td>What is the Dharan culture and traditions?</td>\n      <td>The Dharan Culture and traditions are deeply r...</td>\n      <td>What is the Dharan culture and traditions? The...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>50923</th>\n      <td>Trekking in Nepal</td>\n      <td>What challenges do trekkers face with transpor...</td>\n      <td>Transportation to the starting points of treks...</td>\n      <td>What challenges do trekkers face with transpor...</td>\n    </tr>\n    <tr>\n      <th>50924</th>\n      <td>Trekking in Nepal</td>\n      <td>How many distinct and diverse vegetation zones...</td>\n      <td>Nepal has six distinct and diverse vegetation ...</td>\n      <td>How many distinct and diverse vegetation zones...</td>\n    </tr>\n    <tr>\n      <th>50925</th>\n      <td>Trekking in Nepal</td>\n      <td>Which rare species might be encountered in the...</td>\n      <td>In the higher altitudes, one might encounter r...</td>\n      <td>Which rare species might be encountered in the...</td>\n    </tr>\n    <tr>\n      <th>50926</th>\n      <td>Trekking in Nepal</td>\n      <td>What accommodations are available on popular t...</td>\n      <td>Lodge accommodation is available on popular tr...</td>\n      <td>What accommodations are available on popular t...</td>\n    </tr>\n    <tr>\n      <th>50927</th>\n      <td>Trekking in Nepal</td>\n      <td>Why is hiring a porter considered valuable in ...</td>\n      <td>Hiring a porter is valued in Nepali culture be...</td>\n      <td>Why is hiring a porter considered valuable in ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>50928 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('/kaggle/input/qa_model/transformers/v1/4/neha_kaggle_trained_gpt2/question_answermodel-t5')","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:06:12.538610Z","iopub.execute_input":"2024-03-04T07:06:12.539497Z","iopub.status.idle":"2024-03-04T07:06:13.059060Z","shell.execute_reply.started":"2024-03-04T07:06:12.539462Z","shell.execute_reply":"2024-03-04T07:06:13.057868Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#layer freezing teachnique\nnamed_params = list(model.named_parameters())\ntotal_params = sum(p.numel() for p in model.parameters())\n\n\n# Define the layers to freeze\nlayers_to_freeze = range(120)\n\n#Freeze the specified layer\nfor index, (name, param) in enumerate(named_params):\n    if index in layers_to_freeze:\n        param.requires_grad = False\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n# Print the freezing status and count of trainable parameters\nfor name, param in model.named_parameters():\n    print(f\"{name}, Requires Grad: {param.requires_grad}\")\n\nprint(f\"Total Parameters: {total_params}\")\nprint(f\"Trainable Parameters: {trainable_params}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:06:13.060495Z","iopub.execute_input":"2024-03-04T07:06:13.060734Z","iopub.status.idle":"2024-03-04T07:06:13.091619Z","shell.execute_reply.started":"2024-03-04T07:06:13.060713Z","shell.execute_reply":"2024-03-04T07:06:13.090766Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"transformer.wte.weight, Requires Grad: False\ntransformer.wpe.weight, Requires Grad: False\ntransformer.h.0.ln_1.weight, Requires Grad: False\ntransformer.h.0.ln_1.bias, Requires Grad: False\ntransformer.h.0.attn.c_attn.weight, Requires Grad: False\ntransformer.h.0.attn.c_attn.bias, Requires Grad: False\ntransformer.h.0.attn.c_proj.weight, Requires Grad: False\ntransformer.h.0.attn.c_proj.bias, Requires Grad: False\ntransformer.h.0.ln_2.weight, Requires Grad: False\ntransformer.h.0.ln_2.bias, Requires Grad: False\ntransformer.h.0.mlp.c_fc.weight, Requires Grad: False\ntransformer.h.0.mlp.c_fc.bias, Requires Grad: False\ntransformer.h.0.mlp.c_proj.weight, Requires Grad: False\ntransformer.h.0.mlp.c_proj.bias, Requires Grad: False\ntransformer.h.1.ln_1.weight, Requires Grad: False\ntransformer.h.1.ln_1.bias, Requires Grad: False\ntransformer.h.1.attn.c_attn.weight, Requires Grad: False\ntransformer.h.1.attn.c_attn.bias, Requires Grad: False\ntransformer.h.1.attn.c_proj.weight, Requires Grad: False\ntransformer.h.1.attn.c_proj.bias, Requires Grad: False\ntransformer.h.1.ln_2.weight, Requires Grad: False\ntransformer.h.1.ln_2.bias, Requires Grad: False\ntransformer.h.1.mlp.c_fc.weight, Requires Grad: False\ntransformer.h.1.mlp.c_fc.bias, Requires Grad: False\ntransformer.h.1.mlp.c_proj.weight, Requires Grad: False\ntransformer.h.1.mlp.c_proj.bias, Requires Grad: False\ntransformer.h.2.ln_1.weight, Requires Grad: False\ntransformer.h.2.ln_1.bias, Requires Grad: False\ntransformer.h.2.attn.c_attn.weight, Requires Grad: False\ntransformer.h.2.attn.c_attn.bias, Requires Grad: False\ntransformer.h.2.attn.c_proj.weight, Requires Grad: False\ntransformer.h.2.attn.c_proj.bias, Requires Grad: False\ntransformer.h.2.ln_2.weight, Requires Grad: False\ntransformer.h.2.ln_2.bias, Requires Grad: False\ntransformer.h.2.mlp.c_fc.weight, Requires Grad: False\ntransformer.h.2.mlp.c_fc.bias, Requires Grad: False\ntransformer.h.2.mlp.c_proj.weight, Requires Grad: False\ntransformer.h.2.mlp.c_proj.bias, Requires Grad: False\ntransformer.h.3.ln_1.weight, Requires Grad: False\ntransformer.h.3.ln_1.bias, Requires Grad: False\ntransformer.h.3.attn.c_attn.weight, Requires Grad: False\ntransformer.h.3.attn.c_attn.bias, Requires Grad: False\ntransformer.h.3.attn.c_proj.weight, Requires Grad: False\ntransformer.h.3.attn.c_proj.bias, Requires Grad: False\ntransformer.h.3.ln_2.weight, Requires Grad: False\ntransformer.h.3.ln_2.bias, Requires Grad: False\ntransformer.h.3.mlp.c_fc.weight, Requires Grad: False\ntransformer.h.3.mlp.c_fc.bias, Requires Grad: False\ntransformer.h.3.mlp.c_proj.weight, Requires Grad: False\ntransformer.h.3.mlp.c_proj.bias, Requires Grad: False\ntransformer.h.4.ln_1.weight, Requires Grad: False\ntransformer.h.4.ln_1.bias, Requires Grad: False\ntransformer.h.4.attn.c_attn.weight, Requires Grad: False\ntransformer.h.4.attn.c_attn.bias, Requires Grad: False\ntransformer.h.4.attn.c_proj.weight, Requires Grad: False\ntransformer.h.4.attn.c_proj.bias, Requires Grad: False\ntransformer.h.4.ln_2.weight, Requires Grad: False\ntransformer.h.4.ln_2.bias, Requires Grad: False\ntransformer.h.4.mlp.c_fc.weight, Requires Grad: False\ntransformer.h.4.mlp.c_fc.bias, Requires Grad: False\ntransformer.h.4.mlp.c_proj.weight, Requires Grad: False\ntransformer.h.4.mlp.c_proj.bias, Requires Grad: False\ntransformer.h.5.ln_1.weight, Requires Grad: False\ntransformer.h.5.ln_1.bias, Requires Grad: False\ntransformer.h.5.attn.c_attn.weight, Requires Grad: False\ntransformer.h.5.attn.c_attn.bias, Requires Grad: False\ntransformer.h.5.attn.c_proj.weight, Requires Grad: False\ntransformer.h.5.attn.c_proj.bias, Requires Grad: False\ntransformer.h.5.ln_2.weight, Requires Grad: False\ntransformer.h.5.ln_2.bias, Requires Grad: False\ntransformer.h.5.mlp.c_fc.weight, Requires Grad: False\ntransformer.h.5.mlp.c_fc.bias, Requires Grad: False\ntransformer.h.5.mlp.c_proj.weight, Requires Grad: False\ntransformer.h.5.mlp.c_proj.bias, Requires Grad: False\ntransformer.h.6.ln_1.weight, Requires Grad: False\ntransformer.h.6.ln_1.bias, Requires Grad: False\ntransformer.h.6.attn.c_attn.weight, Requires Grad: False\ntransformer.h.6.attn.c_attn.bias, Requires Grad: False\ntransformer.h.6.attn.c_proj.weight, Requires Grad: False\ntransformer.h.6.attn.c_proj.bias, Requires Grad: False\ntransformer.h.6.ln_2.weight, Requires Grad: False\ntransformer.h.6.ln_2.bias, Requires Grad: False\ntransformer.h.6.mlp.c_fc.weight, Requires Grad: False\ntransformer.h.6.mlp.c_fc.bias, Requires Grad: False\ntransformer.h.6.mlp.c_proj.weight, Requires Grad: False\ntransformer.h.6.mlp.c_proj.bias, Requires Grad: False\ntransformer.h.7.ln_1.weight, Requires Grad: False\ntransformer.h.7.ln_1.bias, Requires Grad: False\ntransformer.h.7.attn.c_attn.weight, Requires Grad: False\ntransformer.h.7.attn.c_attn.bias, Requires Grad: False\ntransformer.h.7.attn.c_proj.weight, Requires Grad: False\ntransformer.h.7.attn.c_proj.bias, Requires Grad: False\ntransformer.h.7.ln_2.weight, Requires Grad: False\ntransformer.h.7.ln_2.bias, Requires Grad: False\ntransformer.h.7.mlp.c_fc.weight, Requires Grad: False\ntransformer.h.7.mlp.c_fc.bias, Requires Grad: False\ntransformer.h.7.mlp.c_proj.weight, Requires Grad: False\ntransformer.h.7.mlp.c_proj.bias, Requires Grad: False\ntransformer.h.8.ln_1.weight, Requires Grad: False\ntransformer.h.8.ln_1.bias, Requires Grad: False\ntransformer.h.8.attn.c_attn.weight, Requires Grad: False\ntransformer.h.8.attn.c_attn.bias, Requires Grad: False\ntransformer.h.8.attn.c_proj.weight, Requires Grad: False\ntransformer.h.8.attn.c_proj.bias, Requires Grad: False\ntransformer.h.8.ln_2.weight, Requires Grad: False\ntransformer.h.8.ln_2.bias, Requires Grad: False\ntransformer.h.8.mlp.c_fc.weight, Requires Grad: False\ntransformer.h.8.mlp.c_fc.bias, Requires Grad: False\ntransformer.h.8.mlp.c_proj.weight, Requires Grad: False\ntransformer.h.8.mlp.c_proj.bias, Requires Grad: False\ntransformer.h.9.ln_1.weight, Requires Grad: False\ntransformer.h.9.ln_1.bias, Requires Grad: False\ntransformer.h.9.attn.c_attn.weight, Requires Grad: False\ntransformer.h.9.attn.c_attn.bias, Requires Grad: False\ntransformer.h.9.attn.c_proj.weight, Requires Grad: False\ntransformer.h.9.attn.c_proj.bias, Requires Grad: False\ntransformer.h.9.ln_2.weight, Requires Grad: False\ntransformer.h.9.ln_2.bias, Requires Grad: False\ntransformer.h.9.mlp.c_fc.weight, Requires Grad: False\ntransformer.h.9.mlp.c_fc.bias, Requires Grad: False\ntransformer.h.9.mlp.c_proj.weight, Requires Grad: True\ntransformer.h.9.mlp.c_proj.bias, Requires Grad: True\ntransformer.h.10.ln_1.weight, Requires Grad: True\ntransformer.h.10.ln_1.bias, Requires Grad: True\ntransformer.h.10.attn.c_attn.weight, Requires Grad: True\ntransformer.h.10.attn.c_attn.bias, Requires Grad: True\ntransformer.h.10.attn.c_proj.weight, Requires Grad: True\ntransformer.h.10.attn.c_proj.bias, Requires Grad: True\ntransformer.h.10.ln_2.weight, Requires Grad: True\ntransformer.h.10.ln_2.bias, Requires Grad: True\ntransformer.h.10.mlp.c_fc.weight, Requires Grad: True\ntransformer.h.10.mlp.c_fc.bias, Requires Grad: True\ntransformer.h.10.mlp.c_proj.weight, Requires Grad: True\ntransformer.h.10.mlp.c_proj.bias, Requires Grad: True\ntransformer.h.11.ln_1.weight, Requires Grad: True\ntransformer.h.11.ln_1.bias, Requires Grad: True\ntransformer.h.11.attn.c_attn.weight, Requires Grad: True\ntransformer.h.11.attn.c_attn.bias, Requires Grad: True\ntransformer.h.11.attn.c_proj.weight, Requires Grad: True\ntransformer.h.11.attn.c_proj.bias, Requires Grad: True\ntransformer.h.11.ln_2.weight, Requires Grad: True\ntransformer.h.11.ln_2.bias, Requires Grad: True\ntransformer.h.11.mlp.c_fc.weight, Requires Grad: True\ntransformer.h.11.mlp.c_fc.bias, Requires Grad: True\ntransformer.h.11.mlp.c_proj.weight, Requires Grad: True\ntransformer.h.11.mlp.c_proj.bias, Requires Grad: True\ntransformer.ln_f.weight, Requires Grad: True\ntransformer.ln_f.bias, Requires Grad: True\nTotal Parameters: 124439808\nTrainable Parameters: 16537344\n","output_type":"stream"}]},{"cell_type":"code","source":"model = model.to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:06:13.482953Z","iopub.execute_input":"2024-03-04T07:06:13.483286Z","iopub.status.idle":"2024-03-04T07:06:13.621860Z","shell.execute_reply.started":"2024-03-04T07:06:13.483262Z","shell.execute_reply":"2024-03-04T07:06:13.620781Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Tokenize the text\ntrain_dataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path='/kaggle/working/MajorFinaltrain1.txt',\n    block_size=128)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=False,\n)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/question_answermodel-gpt2',\n    overwrite_output_dir=True,\n    num_train_epochs=40,\n    logging_dir='/kaggle/working/question_answermodel-gpt2',\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=32,\n    save_steps=10_000,\n    save_total_limit=2,\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n)\n\n# Start fine-tuning\ntrainer.train()\n#trainer.train(resume_from_checkpoint=True)","metadata":{"id":"y4oGhIhwWsWu","outputId":"03f6173d-e07a-4cc9-fb5a-e891d2e6b327","execution":{"iopub.status.busy":"2024-03-04T07:06:14.147681Z","iopub.execute_input":"2024-03-04T07:06:14.148070Z","iopub.status.idle":"2024-03-04T12:05:54.850796Z","shell.execute_reply.started":"2024-03-04T07:06:14.148041Z","shell.execute_reply":"2024-03-04T12:05:54.849841Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='61320' max='61320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [61320/61320 4:59:39, Epoch 40/40]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.283300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.284100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.287100</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.280100</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.282000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.283400</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.276700</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.278600</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.280400</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.276400</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.275500</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.277000</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.273800</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.273600</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.276100</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.271500</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.271800</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.274100</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.270200</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.271400</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.271100</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.267600</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.267900</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.269900</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.267500</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.266200</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.265600</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.266900</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.263400</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.264600</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.263000</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.262100</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.264000</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.262500</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.261700</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.261900</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.263100</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.258200</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>0.260700</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.260800</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>0.256600</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>0.260200</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>0.259400</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.255000</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>0.257400</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>0.260500</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>0.254100</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.256900</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>0.257200</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>0.255100</td>\n    </tr>\n    <tr>\n      <td>25500</td>\n      <td>0.256000</td>\n    </tr>\n    <tr>\n      <td>26000</td>\n      <td>0.255900</td>\n    </tr>\n    <tr>\n      <td>26500</td>\n      <td>0.253800</td>\n    </tr>\n    <tr>\n      <td>27000</td>\n      <td>0.254200</td>\n    </tr>\n    <tr>\n      <td>27500</td>\n      <td>0.254900</td>\n    </tr>\n    <tr>\n      <td>28000</td>\n      <td>0.251800</td>\n    </tr>\n    <tr>\n      <td>28500</td>\n      <td>0.253300</td>\n    </tr>\n    <tr>\n      <td>29000</td>\n      <td>0.255800</td>\n    </tr>\n    <tr>\n      <td>29500</td>\n      <td>0.251500</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>0.253300</td>\n    </tr>\n    <tr>\n      <td>30500</td>\n      <td>0.252300</td>\n    </tr>\n    <tr>\n      <td>31000</td>\n      <td>0.253000</td>\n    </tr>\n    <tr>\n      <td>31500</td>\n      <td>0.252900</td>\n    </tr>\n    <tr>\n      <td>32000</td>\n      <td>0.250900</td>\n    </tr>\n    <tr>\n      <td>32500</td>\n      <td>0.251900</td>\n    </tr>\n    <tr>\n      <td>33000</td>\n      <td>0.250100</td>\n    </tr>\n    <tr>\n      <td>33500</td>\n      <td>0.251400</td>\n    </tr>\n    <tr>\n      <td>34000</td>\n      <td>0.250600</td>\n    </tr>\n    <tr>\n      <td>34500</td>\n      <td>0.248500</td>\n    </tr>\n    <tr>\n      <td>35000</td>\n      <td>0.251700</td>\n    </tr>\n    <tr>\n      <td>35500</td>\n      <td>0.250500</td>\n    </tr>\n    <tr>\n      <td>36000</td>\n      <td>0.250300</td>\n    </tr>\n    <tr>\n      <td>36500</td>\n      <td>0.250100</td>\n    </tr>\n    <tr>\n      <td>37000</td>\n      <td>0.248700</td>\n    </tr>\n    <tr>\n      <td>37500</td>\n      <td>0.249300</td>\n    </tr>\n    <tr>\n      <td>38000</td>\n      <td>0.249600</td>\n    </tr>\n    <tr>\n      <td>38500</td>\n      <td>0.249300</td>\n    </tr>\n    <tr>\n      <td>39000</td>\n      <td>0.248000</td>\n    </tr>\n    <tr>\n      <td>39500</td>\n      <td>0.248900</td>\n    </tr>\n    <tr>\n      <td>40000</td>\n      <td>0.250200</td>\n    </tr>\n    <tr>\n      <td>40500</td>\n      <td>0.246000</td>\n    </tr>\n    <tr>\n      <td>41000</td>\n      <td>0.249000</td>\n    </tr>\n    <tr>\n      <td>41500</td>\n      <td>0.249100</td>\n    </tr>\n    <tr>\n      <td>42000</td>\n      <td>0.247400</td>\n    </tr>\n    <tr>\n      <td>42500</td>\n      <td>0.247900</td>\n    </tr>\n    <tr>\n      <td>43000</td>\n      <td>0.248200</td>\n    </tr>\n    <tr>\n      <td>43500</td>\n      <td>0.248300</td>\n    </tr>\n    <tr>\n      <td>44000</td>\n      <td>0.247800</td>\n    </tr>\n    <tr>\n      <td>44500</td>\n      <td>0.247200</td>\n    </tr>\n    <tr>\n      <td>45000</td>\n      <td>0.246200</td>\n    </tr>\n    <tr>\n      <td>45500</td>\n      <td>0.248500</td>\n    </tr>\n    <tr>\n      <td>46000</td>\n      <td>0.248600</td>\n    </tr>\n    <tr>\n      <td>46500</td>\n      <td>0.246600</td>\n    </tr>\n    <tr>\n      <td>47000</td>\n      <td>0.247800</td>\n    </tr>\n    <tr>\n      <td>47500</td>\n      <td>0.247900</td>\n    </tr>\n    <tr>\n      <td>48000</td>\n      <td>0.245400</td>\n    </tr>\n    <tr>\n      <td>48500</td>\n      <td>0.248600</td>\n    </tr>\n    <tr>\n      <td>49000</td>\n      <td>0.248400</td>\n    </tr>\n    <tr>\n      <td>49500</td>\n      <td>0.246300</td>\n    </tr>\n    <tr>\n      <td>50000</td>\n      <td>0.247400</td>\n    </tr>\n    <tr>\n      <td>50500</td>\n      <td>0.246900</td>\n    </tr>\n    <tr>\n      <td>51000</td>\n      <td>0.247000</td>\n    </tr>\n    <tr>\n      <td>51500</td>\n      <td>0.247700</td>\n    </tr>\n    <tr>\n      <td>52000</td>\n      <td>0.247400</td>\n    </tr>\n    <tr>\n      <td>52500</td>\n      <td>0.247900</td>\n    </tr>\n    <tr>\n      <td>53000</td>\n      <td>0.247400</td>\n    </tr>\n    <tr>\n      <td>53500</td>\n      <td>0.247500</td>\n    </tr>\n    <tr>\n      <td>54000</td>\n      <td>0.246700</td>\n    </tr>\n    <tr>\n      <td>54500</td>\n      <td>0.247700</td>\n    </tr>\n    <tr>\n      <td>55000</td>\n      <td>0.247500</td>\n    </tr>\n    <tr>\n      <td>55500</td>\n      <td>0.247600</td>\n    </tr>\n    <tr>\n      <td>56000</td>\n      <td>0.246700</td>\n    </tr>\n    <tr>\n      <td>56500</td>\n      <td>0.247400</td>\n    </tr>\n    <tr>\n      <td>57000</td>\n      <td>0.247400</td>\n    </tr>\n    <tr>\n      <td>57500</td>\n      <td>0.248600</td>\n    </tr>\n    <tr>\n      <td>58000</td>\n      <td>0.247700</td>\n    </tr>\n    <tr>\n      <td>58500</td>\n      <td>0.246700</td>\n    </tr>\n    <tr>\n      <td>59000</td>\n      <td>0.248600</td>\n    </tr>\n    <tr>\n      <td>59500</td>\n      <td>0.248700</td>\n    </tr>\n    <tr>\n      <td>60000</td>\n      <td>0.247500</td>\n    </tr>\n    <tr>\n      <td>60500</td>\n      <td>0.249100</td>\n    </tr>\n    <tr>\n      <td>61000</td>\n      <td>0.248700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=61320, training_loss=0.25698318493887246, metrics={'train_runtime': 17979.4906, 'train_samples_per_second': 109.133, 'train_steps_per_second': 3.411, 'total_flos': 1.2817419337728e+17, 'train_loss': 0.25698318493887246, 'epoch': 40.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model()","metadata":{"id":"hM8hAdUCdkdg","execution":{"iopub.status.busy":"2024-03-04T12:08:53.630403Z","iopub.execute_input":"2024-03-04T12:08:53.630772Z","iopub.status.idle":"2024-03-04T12:08:54.839331Z","shell.execute_reply.started":"2024-03-04T12:08:53.630742Z","shell.execute_reply":"2024-03-04T12:08:54.838186Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import zipfile\nimport os\nfrom IPython.display import FileLink\n\ndef zip_dir(directory = os.curdir, file_name = 'directory.zip'):\n    os.chdir(directory)\n    zip_ref = zipfile.ZipFile(file_name, mode='w')\n    for folder, _, files in os.walk(directory):\n        for file in files:\n            if file_name in file:\n                pass\n            else:\n                zip_ref.write(os.path.join(folder, file))\n\n    return FileLink(file_name)\nzip_dir()","metadata":{"id":"r_lcBinBXbT_","outputId":"bd3cc2a2-c76c-4630-f9d0-4f4807c421b4","execution":{"iopub.status.busy":"2024-03-04T12:09:55.664980Z","iopub.execute_input":"2024-03-04T12:09:55.665342Z","iopub.status.idle":"2024-03-04T12:09:59.937773Z","shell.execute_reply.started":"2024-03-04T12:09:55.665312Z","shell.execute_reply":"2024-03-04T12:09:59.936682Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/directory.zip","text/html":"<a href='directory.zip' target='_blank'>directory.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel","metadata":{"id":"4E-OE7KHXmkq","execution":{"iopub.status.busy":"2024-03-04T12:10:42.029833Z","iopub.execute_input":"2024-03-04T12:10:42.030617Z","iopub.status.idle":"2024-03-04T12:10:42.036097Z","shell.execute_reply.started":"2024-03-04T12:10:42.030588Z","shell.execute_reply":"2024-03-04T12:10:42.034979Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#from transformers import GPT2Model","metadata":{"execution":{"iopub.execute_input":"2024-02-23T13:52:59.022263Z","iopub.status.busy":"2024-02-23T13:52:59.021845Z","iopub.status.idle":"2024-02-23T13:52:59.028147Z","shell.execute_reply":"2024-02-23T13:52:59.027118Z","shell.execute_reply.started":"2024-02-23T13:52:59.022220Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model_path = '/kaggle/working/question_answermodel-gpt2'\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n\ndef generate_answer(question, max_length=250):\n\n    # Encode the question to tensor of integers using the tokenizer\n    input_ids = tokenizer.encode(question, return_tensors='pt')\n\n    # Generate a sequence of tokens in response to the input question\n    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n\n    # Decode the generated ids to a string\n    answer = tokenizer.decode(output[0], skip_special_tokens=True,temperature=0,top_p=0.85,top_k=20,repetition_penalty=1.5,do_sample=False,early_stopping=True,no_repeat_ngram_size=4)\n    return answer\n\n# Example question\nquestion = \"What is there at Dharan to be considered as a tourist destination?\"\nanswer = generate_answer(question)\nprint(answer)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:17:46.808651Z","iopub.execute_input":"2024-03-04T12:17:46.809398Z","iopub.status.idle":"2024-03-04T12:17:55.182716Z","shell.execute_reply.started":"2024-03-04T12:17:46.809363Z","shell.execute_reply":"2024-03-04T12:17:55.178832Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"What is there at Dharan to be considered as a tourist destination? Dharan, located in the Koshi Zone of eastern Nepal, is a city with a rich cultural heritage and natural beauty that makes it an attractive tourist destination. While it may not be as well-known as other popular tourist spots in Nepal like Kathmandu or Pokhara, Dharan offers a unique experience for visitors.\nOne of the main attractions in Dharan is the Buda Subba Temple, dedicated to Lord Buddha. This temple is perched on a hilltop and offers panoramic views of the city and the surrounding mountains. The serene ambiance of the temple and the beautiful views of the lush green hills make it a must-visit place in Dharan. The city is also known for its cultural richness, with various local festivals and events that showcase the traditions of the region.\nThe cultural diversity in Dharan is also a delight, with the citys showing a blend of different ethnic groups, including the Limbu, Rai, Gurung, and Newar communities. The predominant religion is Hindu and the predominant religion, and you can witness various festivals throughout the year. The locals celebrate Dashain festival of B\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}